name: qwen-vl
base: core24
version: &snap-version "2.5" # Qwen2.5 VL
summary: Qwen Vision Language Model
description: |
  This snap installs an optimized environment for inference with the
  Qwen 2.5 Vision Language Model.

grade: devel
confinement: strict

assumes:
  - snapd2.68 # for components support

compression: lzo

environment:
  # Workaround until it gets set by snapd
  SNAP_COMPONENTS: /snap/$SNAP_INSTANCE_NAME/components/$SNAP_REVISION

layout:
  # Wget's default config file
  # This is to avoid runtime errors and AppArmor denials
  /etc/wgetrc:
    bind-file: $SNAP/etc/wgetrc
  # OpenCL looks in /etc/OpenCL/vendors for .icd files containing the paths to shared objects
  /etc/OpenCL/vendors:
    bind: $SNAP/etc/OpenCL/vendors
  # Intel icd files use an absolute path to Intel OpenCL shared objects. Use a layout to mount the so files at the expected location
  /usr/lib/$CRAFT_ARCH_TRIPLET_BUILD_FOR/intel-opencl:
    bind: $SNAP/usr/lib/$CRAFT_ARCH_TRIPLET_BUILD_FOR/intel-opencl
  # NVIDIA icd specifies only the shared object name: libnvidia-opencl.so.1
  # libnvidia-opencl.so.1 is inside /snap/stack-utils/x3/usr/lib/x86_64-linux-gnu/, which is already in LD_LIBRARY_PATH

plugs:
  intel-npu:
    interface: custom-device
    custom-device: intel-npu-device
  npu-libs:
    interface: content
    content: npu-libs-2404
    target: $SNAP/npu-libs
  # To allow sideloading models by root
  home:
    read: all
    
hooks:
  install:
    environment:
      # To load libigdrcl.so, for Intel GPU vRAM
      OCL_ICD_VENDORS: $SNAP/etc/OpenCL/vendors
    plugs:
      # For hardware detection
      - hardware-observe
      - opengl

parts:
  app-scripts:
    source: apps
    plugin: dump
    organize:
      "*": bin/

  engines:
    source: engines
    plugin: dump
    organize:
      "*": engines/

  cli:
    source: cli # this is a private git submodule which must be cloned separately
    plugin: go
    build-snaps:
      - go/1.24/stable
    stage-packages:
      - pciutils
      - nvidia-utils # for Nvidia vRAM and Cuda Capability detection
      - clinfo # for Intel GPU vRAM detection
    organize:
      bin/cli: bin/qwen-vl

  go-chat-client:
    source: https://github.com/jpm-canonical/go-chat-client.git
    source-type: git
    source-tag: v1.0.0-beta.1
    plugin: go
    build-snaps:
      - go/1.24/stable

  common-runtime-dependencies:
    plugin: nil
    stage-packages:
      - wget # model init
      - jq # install hook
    stage-snaps:
      - yq # install hook

  llamacpp: &llamacpp-part
    source: https://github.com/ggerganov/llama.cpp.git
    source-tag: &llamacpp-tag b5794
    source-depth: 1
    plugin: cmake
    cmake-parameters:
      - -DLLAMA_CURL=OFF
      - -DGGML_NATIVE=OFF
    stage-packages:
      - libgomp1
    organize:
      # move everything, including the staged packages
      "*": (component/llamacpp)

  llamacpp-avx512:
    <<: *llamacpp-part
    cmake-parameters:
      - -DLLAMA_CURL=OFF
      - -DGGML_NATIVE=OFF
      - -DGGML_AVX512=ON
    organize:
      "*": (component/llamacpp-avx512)

  llamacpp-cuda:
    <<: *llamacpp-part
    build-packages:
      - to amd64: # see also the overrides
        - nvidia-cuda-toolkit
    # stage-packages:
    #   - libgomp1
    #   - libcudart12
    #   - libcublas12
    cmake-parameters:
      - -DLLAMA_CURL=OFF
      - -DGGML_NATIVE=OFF
      - -DGGML_CUDA=ON
      - -DCMAKE_CUDA_ARCHITECTURES=all-major
    override-pull: &amd64-only |
      if [ "$CRAFT_ARCH_BUILD_FOR" == "amd64" ]; then
        craftctl default
      fi
    override-build: *amd64-only
    override-stage: *amd64-only
    override-prime: *amd64-only
    organize:
      "*": (component/llamacpp-cuda)

  llama-aio: &llama-aio-part
    source: https://github.com/AmpereComputingAI/llama.cpp/releases/download/v3.2.1/llama_aio_v3.2.1_b322a87.tar.gz
    source-type: tar
    plugin: dump
    stage-packages:
      - libcurl4
    organize:
      "LICENSE*.txt": usr/share/doc/llama-aio/
      llama-server: (component/llama-aio)/bin/
      libllama.so: (component/llama-aio)/lib/
      "libggml*.so": (component/llama-aio)/lib/
      libmtmd.so: (component/llama-aio)/lib/
      "usr/lib/$CRAFT_ARCH_TRIPLET_BUILD_FOR/": (component/llama-aio)/lib/
    prime:
      - -*

  llama-aio-ampereone:
    <<: *llama-aio-part
    source: https://github.com/AmpereComputingAI/llama.cpp/releases/download/v3.2.1/llama_aio_v3.2.1_b322a87_ampereone.tar.gz
    organize:
      "LICENSE*.txt": usr/share/doc/llama-aio-ampereone/
      llama-server: (component/llama-aio-ampereone)/bin/
      libllama.so: (component/llama-aio-ampereone)/lib/
      "libggml*.so": (component/llama-aio-ampereone)/lib/
      libmtmd.so: (component/llama-aio-ampereone)/lib/
      "usr/lib/$CRAFT_ARCH_TRIPLET_BUILD_FOR/": (component/llama-aio-ampereone)/lib/

  model-qwen2-5-vl-3b-instruct-q4-k-m:
    source: components/model-qwen2-5-vl-3b-instruct-q4-k-m
    plugin: dump
    organize:
      "*": (component/model-qwen2-5-vl-3b-instruct-q4-k-m)

  mmproj-qwen2-5-vl-3b-instruct-q8-0:
    source: components/mmproj-qwen2-5-vl-3b-instruct-q8-0
    plugin: dump
    organize:
      "*": (component/mmproj-qwen2-5-vl-3b-instruct-q8-0)

  model-qwen2-5-vl-7b-instruct-q4-k-m:
    source: components/model-qwen2-5-vl-7b-instruct-q4-k-m
    plugin: dump
    organize:
      "*": (component/model-qwen2-5-vl-7b-instruct-q4-k-m)

  mmproj-qwen2-5-vl-7b-instruct-q8-0:
    source: components/mmproj-qwen2-5-vl-7b-instruct-q8-0
    plugin: dump
    organize:
      "*": (component/mmproj-qwen2-5-vl-7b-instruct-q8-0)

  model-qwen2-5-vl-3b-instruct-aio-q8r16:
    source: components/model-qwen2-5-vl-3b-instruct-aio-q8r16
    plugin: dump
    organize:
      "*": (component/model-qwen2-5-vl-3b-instruct-aio-q8r16)

  mmproj-qwen2-5-vl-3b-instruct-aio-q8-0:
    source: components/mmproj-qwen2-5-vl-3b-instruct-aio-q8-0
    plugin: dump
    organize:
      "*": (component/mmproj-qwen2-5-vl-3b-instruct-aio-q8-0)

  model-qwen2-5-vl-3b-instruct-ov-int4:
    source: components/model-qwen2-5-vl-3b-instruct-ov-int4
    plugin: dump
    override-build: |
      # At startup OVMS (single model mode) creates the mediapipe graph file inside the model directory.
      # This directory is read-only when it is inside a component.
      # By adding a symlink with the file's name inside the model directory, pointing to /tmp,
      # OVMS will follow the symlink during runtime and instead create the file in /tmp.
      mediapipe_graph_path="Qwen2.5-VL-3B-Instruct-ov-int4/graph.pbtxt"
      if [ ! -L "$mediapipe_graph_path" ]; then
        ln -s /tmp/graph.pbtxt "$mediapipe_graph_path"
      fi

      craftctl default
    organize:
      "*": (component/model-qwen2-5-vl-3b-instruct-ov-int4)

  model-qwen2-5-vl-3b-instruct-ov-int4-npu:
    source: components/model-qwen2-5-vl-3b-instruct-ov-int4-npu
    plugin: dump
    override-build: |
      mediapipe_graph_path="Qwen2.5-VL-3B-Instruct-ov-int4-npu/graph.pbtxt"
      if [ ! -L "$mediapipe_graph_path" ]; then
        ln -s /tmp/graph.pbtxt "$mediapipe_graph_path"
      fi

      craftctl default
    organize:
      "*": (component/model-qwen2-5-vl-3b-instruct-ov-int4-npu)

  opencl-driver:
    # Includes the OpenCL runtime for Intel GPU support
    plugin: nil
    build-packages:
      - wget
    override-build: |
      # Only install intel opencl drivers on amd64
      if [ "$CRAFT_ARCH_BUILD_FOR" == "amd64" ]; then
        # Legacy Intel OpenCL Runtime
        mkdir -p $CRAFT_PART_BUILD/opencl-intel-legacy1
        cd $CRAFT_PART_BUILD/opencl-intel-legacy1
        wget https://github.com/intel/compute-runtime/releases/download/24.35.30872.36/intel-level-zero-gpu-legacy1_1.5.30872.36_amd64.deb
        wget https://github.com/intel/compute-runtime/releases/download/24.35.30872.36/intel-opencl-icd-legacy1_24.35.30872.36_amd64.deb
        dpkg --root=$CRAFT_PART_INSTALL --force-all -i *.deb
      
        # Recent Intel OpenCL Runtime
        mkdir -p $CRAFT_PART_BUILD/opencl-intel
        cd $CRAFT_PART_BUILD/opencl-intel
        wget https://github.com/intel/intel-graphics-compiler/releases/download/v2.5.6/intel-igc-core-2_2.5.6+18417_amd64.deb
        wget https://github.com/intel/intel-graphics-compiler/releases/download/v2.5.6/intel-igc-opencl-2_2.5.6+18417_amd64.deb
        wget https://github.com/intel/compute-runtime/releases/download/24.52.32224.5/intel-level-zero-gpu_1.6.32224.5_amd64.deb
        wget https://github.com/intel/compute-runtime/releases/download/24.52.32224.5/intel-opencl-icd_24.52.32224.5_amd64.deb
        wget https://github.com/intel/compute-runtime/releases/download/24.52.32224.5/libigdgmm12_22.5.5_amd64.deb
        dpkg --root=$CRAFT_PART_INSTALL --force-all -i *.deb
      
        # Two symlinks are created by installing opencl: 
        # etc/alternatives/ocloc -> /usr/bin/ocloc-24.52.1, usr/bin/ocloc -> /etc/alternatives/ocloc
        # These point to outside of the snap and fails store review. They are not required, so remove it.
        rm $CRAFT_PART_INSTALL/etc/alternatives/ocloc
        rm $CRAFT_PART_INSTALL/usr/bin/ocloc
      
        cd $CRAFT_PART_BUILD
      fi # End of intel-opencl driver installation
      
      craftctl default

  openvino-model-server:
    plugin: dump
    source: https://github.com/openvinotoolkit/model_server/releases/download/v2025.3/ovms_ubuntu24_python_on.tar.gz
    source-checksum: sha256/64e9c3f4b3bdfcd9e33e7319d17cd2a30c110bb8e33991413b6f51921cafe845
    organize:
      "*": (component/openvino-model-server)
    stage-packages:
      - libxml2
      - curl
      - libpython3.12

  openvino-model-server-python-dependencies:
    plugin: python
    source: components/openvino-model-server
    python-packages:
      - Jinja2==3.1.6
      - MarkupSafe==3.0.2
    organize:
      "*": (component/openvino-model-server)

components:
  #
  # Engines
  #

  llamacpp:
    type: standard
    summary: llama.cpp Engine using default CPU instruction sets
    description: LLM inference in C/C++
    version: *llamacpp-tag

  llamacpp-avx512:
    type: standard
    summary: llama.cpp Engine using default and AVX512 CPU instruction sets
    description: LLM inference in C/C++
    version: *llamacpp-tag

  llamacpp-cuda:
    type: standard
    summary: llama.cpp Engine for NVIDIA GPUs using CUDA
    description: LLM inference in C/C++
    version: *llamacpp-tag

  llama-aio:
    type: standard
    summary: llama-aio Engine for Ampere Altra CPUs
    description: LLM inference in C/C++
    version: v3.2.1

  llama-aio-ampereone:
    type: standard
    summary: llama-aio Engine for Ampere One CPUs
    description: LLM inference in C/C++
    version: v3.2.1

  openvino-model-server:
    type: standard
    summary: OpenVINO Model Server
    description: OpenVINO Model Server for serving models
    version: v2025.3

  # 
  # Models
  #

  model-qwen2-5-vl-3b-instruct-q4-k-m:
    type: standard
    summary: Qwen 2.5 VL 3B Q4
    description: Quantized model with 3B parameters in gguf format with Q4_K_M weight encoding
    version: *snap-version

  mmproj-qwen2-5-vl-3b-instruct-q8-0:
    type: standard
    summary: MMProj for Qwen 2.5 VL 3B
    description: Multimodal projector for Qwen 2.5 VL 3B
    version: *snap-version

  model-qwen2-5-vl-7b-instruct-q4-k-m:
    type: standard
    summary: Qwen 2.5 VL 7B Q4
    description: Quantized model with 7B parameters in gguf format with Q4_K_M weight encoding
    version: *snap-version

  mmproj-qwen2-5-vl-7b-instruct-q8-0:
    type: standard
    summary: MMProj Qwen 2.5 VL 7B
    description: Multimodal projector for Qwen 2.5 VL 7B
    version: *snap-version

  model-qwen2-5-vl-3b-instruct-aio-q8r16:
    type: standard
    summary: Qwen 2.5 VL 3B Q8_0 for Ampere CPUs
    description: Quantized model with 3B parameters in gguf format with Q8R16 weight encoding
    version: *snap-version

  mmproj-qwen2-5-vl-3b-instruct-aio-q8-0:
    type: standard
    summary: MMProj Qwen 2.5 VL 3B Q8_0 for Ampere CPUs
    description: Multimodal projector for Qwen 2.5 VL 3B with Q8_0 weight encoding
    version: *snap-version

  model-qwen2-5-vl-3b-instruct-ov-int4:
    type: standard
    summary: Qwen 2.5 VL 3B for Intel CPU and Intel GPU
    description: Model with 3B parameters in OpenVINO IR format, quantized with INT4
    version: *snap-version

  model-qwen2-5-vl-3b-instruct-ov-int4-npu:
    type: standard
    summary: Qwen 2.5 VL 3B for Intel NPU
    description: Model with 3B parameters in OpenVINO IR format, quantized with INT4 and symmetric channel wise quantization to run on Intel NPU
    version: *snap-version

apps:
  qwen-vl:
    command: bin/qwen-vl
    completer: bin/completion.bash
    plugs:
      # For hardware detection
      - opengl
      - hardware-observe
      # To access server over network socket
      - network
    environment:
      # To load libigdrcl.so, for Intel GPU vRAM
      OCL_ICD_VENDORS: $SNAP/etc/OpenCL/vendors
      CHAT: $SNAP/bin/chat.sh

  server:
    command: bin/server.sh
    daemon: simple
    plugs:
      # For inference and hardware detection (via init)
      - opengl
      # Needed by for cuda-uvmfd to listen on a unix socket?
      # Needed for server app by inheritance
      - network-bind
      # Needed to download resources
      - network
      # For hardware detection (via init)
      - hardware-observe
      # For sideloading models
      - home 
      # Intel NPU access (device node)
      - intel-npu 
      # Intel NPU access (libs) 
      - npu-libs  
    environment:
      # Needed for shared libraries used by llama.cpp
      # TODO: Should this instead be added in an env file to llamacpp component
      ARCH_TRIPLET: $CRAFT_ARCH_TRIPLET_BUILD_FOR
